{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VZc01H2RlvXc",
        "q6rWxc7Llw9X"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "VZc01H2RlvXc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6yeH-epll-d",
        "outputId": "f3d5727c-163b-4eb6-d313-4a60005fb871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tokenizers, xxhash, urllib3, multiprocess, responses, huggingface-hub, transformers, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 evaluate-0.4.0 huggingface-hub-0.12.0 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.2 transformers-4.26.1 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets evaluate transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ty7U9ImlrOj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `wget`"
      ],
      "metadata": {
        "id": "q6rWxc7Llw9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/gpt2/model.py' -O 'gpt2.py'\n",
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/gpt2/data/test.json'\n",
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/gpt2/data/train.json'\n",
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/gpt2/data/valid.json'\n",
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/run_glue.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZpGdDBlyAp",
        "outputId": "0e89433a-9788-4e4b-d8c9-43ecfd33a44d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-13 22:50:08--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/gpt2/model.py\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5432 (5.3K) [text/plain]\n",
            "Saving to: ‘gpt2.py’\n",
            "\n",
            "gpt2.py             100%[===================>]   5.30K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-02-13 22:50:10 (3.79 MB/s) - ‘gpt2.py’ saved [5432/5432]\n",
            "\n",
            "--2023-02-13 22:50:10--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/gpt2/data/test.json\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19695881 (19M) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>]  18.78M  2.82MB/s    in 7.7s    \n",
            "\n",
            "2023-02-13 22:50:19 (2.43 MB/s) - ‘test.json’ saved [19695881/19695881]\n",
            "\n",
            "--2023-02-13 22:50:19--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/gpt2/data/train.json\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6385138 (6.1M) [text/plain]\n",
            "Saving to: ‘train.json’\n",
            "\n",
            "train.json          100%[===================>]   6.09M  2.05MB/s    in 3.0s    \n",
            "\n",
            "2023-02-13 22:50:23 (2.05 MB/s) - ‘train.json’ saved [6385138/6385138]\n",
            "\n",
            "--2023-02-13 22:50:23--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/gpt2/data/valid.json\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 896854 (876K) [text/plain]\n",
            "Saving to: ‘valid.json’\n",
            "\n",
            "valid.json          100%[===================>] 875.83K   690KB/s    in 1.3s    \n",
            "\n",
            "2023-02-13 22:50:25 (690 KB/s) - ‘valid.json’ saved [896854/896854]\n",
            "\n",
            "--2023-02-13 22:50:25--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/run_glue.py\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30419 (30K) [text/plain]\n",
            "Saving to: ‘run_glue.py’\n",
            "\n",
            "run_glue.py         100%[===================>]  29.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-13 22:50:26 (124 MB/s) - ‘run_glue.py’ saved [30419/30419]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `run_glue.py`"
      ],
      "metadata": {
        "id": "h7aYzOiDlybr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python run_glue.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path gpt2 \\\n",
        "  --custom_model gpt2_custom \\\n",
        "  --train_file train.json  \\\n",
        "  --validation_file valid.json \\\n",
        "  --test_file test.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --max_seq_length 256 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --max_eval_samples 2000 \\\n",
        "  --num_train_epochs 2 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 250 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 100 \\\n",
        "  --eval_steps 250 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model accuracy \\\n",
        "  --greater_is_better True \\\n",
        "  --load_best_model_at_end True \\\n",
        "  --output_dir drive/MyDrive/out/gpt2 \\\n",
        "  --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4XvbW9flzzf",
        "outputId": "8fb84315-d921-44d4-af01-6075db172d61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-13 23:24:22.843271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-13 23:24:23.685366: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-13 23:24:23.685483: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-13 23:24:23.685503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=250,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=drive/MyDrive/out/gpt2/runs/Feb13_23-24-26_313a88dddaa4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=drive/MyDrive/out/gpt2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=drive/MyDrive/out/gpt2,\n",
            "save_on_each_node=False,\n",
            "save_steps=250,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: train.json\n",
            "INFO:__main__:load a local file for validation: valid.json\n",
            "INFO:__main__:load a local file for test: test.json\n",
            "WARNING:datasets.builder:Using custom data configuration default-61b23d155cb5c1de\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from .cache_training/json/default-61b23d155cb5c1de/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "WARNING:datasets.builder:Found cached dataset json (/content/.cache_training/json/default-61b23d155cb5c1de/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "INFO:datasets.info:Loading Dataset info from /content/.cache_training/json/default-61b23d155cb5c1de/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "100% 3/3 [00:00<00:00, 135.63it/s]\n",
            "[INFO|configuration_utils.py:660] 2023-02-13 23:24:30,192 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-13 23:24:30,193 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:458] 2023-02-13 23:24:31,102 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:660] 2023-02-13 23:24:32,006 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-13 23:24:32,006 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 23:24:33,837 >> loading file vocab.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 23:24:33,837 >> loading file merges.txt from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 23:24:33,837 >> loading file tokenizer.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 23:24:33,837 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 23:24:33,837 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 23:24:33,837 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:660] 2023-02-13 23:24:33,838 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-13 23:24:33,838 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "INFO:__main__:Using hidden states in model: False\n",
            "INFO:__main__:Using implementation from class: GPT2CustomClassifier\n",
            "[INFO|modeling_utils.py:2275] 2023-02-13 23:24:33,901 >> loading weights file pytorch_model.bin from cache at .cache_training/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2857] 2023-02-13 23:24:37,335 >> All model checkpoint weights were used when initializing GPT2CustomClassifier.\n",
            "\n",
            "[WARNING|modeling_utils.py:2859] 2023-02-13 23:24:37,335 >> Some weights of GPT2CustomClassifier were not initialized from the model checkpoint at gpt2 and are newly initialized: ['gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.10.ln_1.bias', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.2.ln_2.bias', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.2.ln_1.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.11.ln_2.bias', 'gpt2.h.9.ln_1.bias', 'gpt2.h.1.attn.bias', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.6.attn.bias', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.7.ln_1.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.4.ln_1.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.7.attn.bias', 'gpt2.h.0.ln_2.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.h.11.ln_1.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.h.8.ln_2.weight', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.2.ln_1.weight', 'gpt2.h.7.ln_2.weight', 'gpt2.h.10.ln_2.weight', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.wpe.weight', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.7.ln_1.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.10.ln_2.bias', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.ln_f.bias', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.4.attn.bias', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.6.ln_2.weight', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.wte.weight', 'gpt2.h.3.ln_2.weight', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.0.attn.bias', 'gpt2.h.5.attn.bias', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.4.ln_2.bias', 'gpt2.h.9.attn.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.9.ln_2.bias', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.10.attn.bias', 'linear.weight', 'gpt2.h.2.ln_2.weight', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.3.ln_1.weight', 'gpt2.h.9.attn.c_attn.weight', 'out_proj.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.1.ln_2.bias', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.11.attn.bias', 'gpt2.h.5.ln_2.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.3.ln_2.bias', 'gpt2.h.0.ln_1.bias', 'gpt2.h.8.ln_2.bias', 'gpt2.h.5.ln_1.bias', 'gpt2.ln_f.weight', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.2.attn.bias', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.8.attn.bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.10.ln_1.weight', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.1.ln_2.weight', 'out_proj.weight', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.4.ln_1.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.6.ln_1.bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.9.ln_2.weight', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.4.attn.c_attn.bias', 'linear.bias', 'gpt2.h.5.ln_2.weight', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.6.ln_1.weight', 'score.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.3.attn.bias', 'gpt2.h.8.ln_1.weight', 'gpt2.h.4.ln_2.weight', 'gpt2.h.1.ln_1.bias', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.9.ln_1.weight', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.11.ln_1.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[ERROR|tokenization_utils_base.py:1042] 2023-02-13 23:24:37,364 >> Using pad_token, but it is not set yet.\n",
            "INFO:__main__:Set PAD token to EOS: <|endoftext|>\n",
            "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-61b23d155cb5c1de/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ccd3e7c768ec001f.arrow\n",
            "Running tokenizer on dataset: 100% 9/9 [00:03<00:00,  2.56ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-61b23d155cb5c1de/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3e6dc7cf2e77fcde.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  2.41ba/s]\n",
            "Running tokenizer on dataset:   0% 0/28 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-61b23d155cb5c1de/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e53a943b76219399.arrow\n",
            "Running tokenizer on dataset: 100% 28/28 [00:12<00:00,  2.21ba/s]\n",
            "INFO:__main__:Set 420 samples for 2-class\n",
            "INFO:__main__:Set 420 samples for 1-class\n",
            "INFO:__main__:Set 420 samples for 0-class\n",
            "INFO:__main__:Sample 1824 of the training set: {'label': 1, 'text': \"Poured from a bottle into my pewter Jagerstein at refrigetor chilled temperature. I peaked a look at the color from my friends glass and found it to be somewhere between amber and stout/porter black. The kind of color that would decieve the average drinker into thinking that this was going to be a beer as thick as motor oil. But that's not how they roll in VT. The smell betrayed a sweeter character, almost over emphasizing the malt properties. The taste was a bit on the hoppy side. That is to say that, the aftertaste left you remembering more of the hops than the also very strong, caramel malt taste. The mouthfeel was perfect for a cool summer night. This would probably be a great beer from late April to early June and late September to early November. I could drink a few of these at a time, but would probably want to move on to something lighter after that.\", 'input_ids': [47, 8167, 422, 257, 9294, 656, 616, 279, 413, 353, 449, 3536, 5714, 379, 1006, 4359, 316, 273, 45550, 5951, 13, 314, 33948, 257, 804, 379, 262, 3124, 422, 616, 2460, 5405, 290, 1043, 340, 284, 307, 7382, 1022, 36505, 290, 39171, 14, 26634, 2042, 13, 383, 1611, 286, 3124, 326, 561, 875, 12311, 262, 2811, 4144, 263, 656, 3612, 326, 428, 373, 1016, 284, 307, 257, 6099, 355, 6546, 355, 5584, 3056, 13, 887, 326, 338, 407, 703, 484, 4836, 287, 32751, 13, 383, 8508, 26281, 257, 3490, 2357, 2095, 11, 2048, 625, 36360, 262, 26868, 6608, 13, 383, 6938, 373, 257, 1643, 319, 262, 8169, 14097, 1735, 13, 1320, 318, 284, 910, 326, 11, 262, 706, 83, 4594, 1364, 345, 24865, 517, 286, 262, 29438, 621, 262, 635, 845, 1913, 11, 32913, 26868, 6938, 13, 383, 5422, 36410, 373, 2818, 329, 257, 3608, 3931, 1755, 13, 770, 561, 2192, 307, 257, 1049, 6099, 422, 2739, 3035, 284, 1903, 2795, 290, 2739, 2693, 284, 1903, 3389, 13, 314, 714, 4144, 257, 1178, 286, 777, 379, 257, 640, 11, 475, 561, 2192, 765, 284, 1445, 319, 284, 1223, 14871, 706, 326, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 409 of the training set: {'label': 1, 'text': 'This beer isn\\'t bad, but there really isn\\'t anything I can say to strongly recommend it; there aren\\'t any really distinctive or unique flavors and I thought I detected a faint flavor defect or two. Organic Revolution doesn\\'t do anything to change my impression that in general the quality (or at least degree of \"interesting\") organic beers remains distinctly below that of non-organic beers. Pours a thick gold lager-color with a very tacky white head that is loosely packed into the top of my glass. Appearance of the beer\\'s body is a bit disconcerting - it has an oily-swirlyness to it. Aroma is only slightly hoppy, with some fruity vinous-ness. Flavor is balanced, but a bit dull with a very faint hoppiness. Some vinous-ness and diacetyl are evident to me.', 'input_ids': [1212, 6099, 2125, 470, 2089, 11, 475, 612, 1107, 2125, 470, 1997, 314, 460, 910, 284, 7634, 4313, 340, 26, 612, 3588, 470, 597, 1107, 18778, 393, 3748, 17361, 290, 314, 1807, 314, 12326, 257, 18107, 9565, 11855, 393, 734, 13, 31734, 9303, 1595, 470, 466, 1997, 284, 1487, 616, 10647, 326, 287, 2276, 262, 3081, 357, 273, 379, 1551, 4922, 286, 366, 47914, 4943, 10469, 16800, 3793, 30911, 2174, 326, 286, 1729, 12, 36617, 16800, 13, 350, 4662, 257, 6546, 3869, 300, 3536, 12, 8043, 351, 257, 845, 6331, 88, 2330, 1182, 326, 318, 28845, 11856, 656, 262, 1353, 286, 616, 5405, 13, 43436, 286, 262, 6099, 338, 1767, 318, 257, 1643, 595, 48415, 278, 532, 340, 468, 281, 44560, 12, 2032, 343, 306, 1108, 284, 340, 13, 317, 42902, 318, 691, 4622, 8169, 14097, 11, 351, 617, 12658, 414, 410, 29823, 12, 1108, 13, 26438, 318, 12974, 11, 475, 257, 1643, 19222, 351, 257, 845, 18107, 8169, 381, 1272, 13, 2773, 410, 29823, 12, 1108, 290, 2566, 23253, 2645, 389, 10678, 284, 502, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 4506 of the training set: {'label': 2, 'text': \"On tap at the brewery on 6/28/09. Pours solid black with mocha head, great lacing. Aroma is certainly dominated by chocolate, but gives way to smoke and coffee. This is a straight up sweet, smooth, balanced stout with a great chocolate finish, and for the style, it's incredible. Will be looking for this on the east coast.\", 'input_ids': [2202, 9814, 379, 262, 22120, 319, 718, 14, 2078, 14, 2931, 13, 350, 4662, 4735, 2042, 351, 285, 5374, 64, 1182, 11, 1049, 300, 4092, 13, 317, 42902, 318, 3729, 13354, 416, 11311, 11, 475, 3607, 835, 284, 7523, 290, 6891, 13, 770, 318, 257, 3892, 510, 6029, 11, 7209, 11, 12974, 39171, 351, 257, 1049, 11311, 5461, 11, 290, 329, 262, 3918, 11, 340, 338, 8082, 13, 2561, 307, 2045, 329, 428, 319, 262, 7627, 7051, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:710] 2023-02-13 23:24:59,192 >> The following columns in the training set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1650] 2023-02-13 23:24:59,204 >> ***** Running training *****\n",
            "[INFO|trainer.py:1651] 2023-02-13 23:24:59,204 >>   Num examples = 9000\n",
            "[INFO|trainer.py:1652] 2023-02-13 23:24:59,204 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1653] 2023-02-13 23:24:59,204 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1654] 2023-02-13 23:24:59,204 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1655] 2023-02-13 23:24:59,204 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1656] 2023-02-13 23:24:59,204 >>   Total optimization steps = 2250\n",
            "[INFO|trainer.py:1657] 2023-02-13 23:24:59,206 >>   Number of trainable parameters = 249474819\n",
            "{'loss': 0.9228, 'learning_rate': 1.9111111111111113e-05, 'epoch': 0.09}\n",
            "{'loss': 0.9136, 'learning_rate': 1.8222222222222224e-05, 'epoch': 0.18}\n",
            " 11% 250/2250 [01:47<14:11,  2.35it/s][INFO|trainer.py:710] 2023-02-13 23:26:46,808 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:26:46,810 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:26:46,810 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:26:46,810 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:08, 14.14it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  9.12it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  8.19it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:13,  7.88it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:13,  7.74it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:13,  7.65it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.49it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.44it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.36it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.31it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:13,  7.32it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:13,  7.33it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.26it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.23it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.21it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.18it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.14it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:13,  7.22it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:12,  7.28it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:12,  7.31it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.34it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.36it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.31it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.24it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.20it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:11,  7.23it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.23it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.20it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.21it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.20it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.17it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.20it/s]\u001b[A\n",
            " 32% 37/116 [00:04<00:10,  7.19it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:10,  7.19it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:10,  7.32it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.34it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.33it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.29it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.26it/s]\u001b[A\n",
            " 38% 44/116 [00:05<00:09,  7.22it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:09,  7.20it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.22it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.22it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.23it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.27it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.24it/s]\u001b[A\n",
            " 44% 51/116 [00:06<00:09,  7.20it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:08,  7.21it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.20it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.24it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.32it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.25it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:07,  7.25it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:07,  7.24it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.10it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.12it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.15it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.18it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.28it/s]\u001b[A\n",
            " 57% 66/116 [00:08<00:06,  7.19it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.12it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.06it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.05it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.10it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.10it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.19it/s]\u001b[A\n",
            " 63% 73/116 [00:09<00:05,  7.22it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.25it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.20it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.16it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.17it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  7.20it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  7.21it/s]\u001b[A\n",
            " 69% 80/116 [00:10<00:04,  7.24it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.24it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.19it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  7.18it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  7.18it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  7.20it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  7.20it/s]\u001b[A\n",
            " 75% 87/116 [00:11<00:04,  7.24it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.20it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.23it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.27it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.27it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  7.25it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  7.21it/s]\u001b[A\n",
            " 81% 94/116 [00:12<00:03,  7.27it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.22it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.19it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.20it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  7.19it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  7.28it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.23it/s]\u001b[A\n",
            " 87% 101/116 [00:13<00:02,  7.21it/s]\u001b[A\n",
            " 88% 102/116 [00:13<00:01,  7.22it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.21it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.20it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.20it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.23it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  7.25it/s]\u001b[A\n",
            " 93% 108/116 [00:14<00:01,  7.23it/s]\u001b[A\n",
            " 94% 109/116 [00:14<00:00,  7.26it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.26it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.23it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.27it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.25it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.24it/s]\u001b[A\n",
            " 99% 115/116 [00:15<00:00,  7.17it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9540964961051941, 'eval_accuracy': 0.4989200830459595, 'eval_runtime': 16.0323, 'eval_samples_per_second': 57.759, 'eval_steps_per_second': 7.235, 'epoch': 0.22}\n",
            " 11% 250/2250 [02:03<14:11,  2.35it/s]\n",
            "100% 116/116 [00:15<00:00,  7.78it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:27:02,844 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-250\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:27:02,849 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-250/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:27:06,715 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:27:07,315 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:27:07,318 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-250/special_tokens_map.json\n",
            "{'loss': 0.8722, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.27}\n",
            "{'loss': 0.8496, 'learning_rate': 1.6444444444444444e-05, 'epoch': 0.36}\n",
            "{'loss': 0.8128, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.44}\n",
            " 22% 500/2250 [03:59<12:27,  2.34it/s][INFO|trainer.py:710] 2023-02-13 23:28:58,784 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:28:58,786 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:28:58,786 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:28:58,786 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:07, 14.32it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  9.10it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  8.09it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:13,  7.79it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:13,  7.72it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:14,  7.63it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.50it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.40it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.30it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.21it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:14,  7.16it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:14,  7.17it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:13,  7.18it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.30it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.36it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.32it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.27it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.25it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:12,  7.30it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:12,  7.24it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:12,  7.24it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.22it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.26it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.22it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.22it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.22it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:11,  7.21it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.23it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.22it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.26it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.24it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.21it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.22it/s]\u001b[A\n",
            " 32% 37/116 [00:04<00:10,  7.20it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:10,  7.17it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:10,  7.17it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.21it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.20it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.27it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.24it/s]\u001b[A\n",
            " 38% 44/116 [00:05<00:09,  7.24it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:09,  7.29it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.27it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.28it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.22it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.20it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.19it/s]\u001b[A\n",
            " 44% 51/116 [00:06<00:09,  7.19it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:08,  7.20it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.22it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.23it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.23it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.22it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.22it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:08,  7.22it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.23it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.20it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.21it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.21it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.16it/s]\u001b[A\n",
            " 57% 66/116 [00:09<00:06,  7.17it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.22it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.21it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.19it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.23it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.19it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.14it/s]\u001b[A\n",
            " 63% 73/116 [00:09<00:06,  7.13it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.17it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.19it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.24it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.25it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  7.30it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  7.25it/s]\u001b[A\n",
            " 69% 80/116 [00:10<00:04,  7.23it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.24it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  7.18it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  7.20it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  7.21it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  7.18it/s]\u001b[A\n",
            " 75% 87/116 [00:11<00:04,  7.14it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.16it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.20it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.16it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.21it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  7.22it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  7.16it/s]\u001b[A\n",
            " 81% 94/116 [00:12<00:03,  7.12it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.18it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.10it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.09it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  7.13it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  7.24it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.26it/s]\u001b[A\n",
            " 87% 101/116 [00:13<00:02,  7.20it/s]\u001b[A\n",
            " 88% 102/116 [00:14<00:01,  7.19it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.16it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.16it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.14it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.18it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  7.25it/s]\u001b[A\n",
            " 93% 108/116 [00:14<00:01,  7.21it/s]\u001b[A\n",
            " 94% 109/116 [00:14<00:00,  7.22it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.19it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.17it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.19it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.17it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.21it/s]\u001b[A\n",
            " 99% 115/116 [00:15<00:00,  7.21it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8585476875305176, 'eval_accuracy': 0.5896328091621399, 'eval_runtime': 16.0517, 'eval_samples_per_second': 57.689, 'eval_steps_per_second': 7.227, 'epoch': 0.44}\n",
            " 22% 500/2250 [04:15<12:27,  2.34it/s]\n",
            "100% 116/116 [00:15<00:00,  7.87it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:29:14,842 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-500\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:29:14,848 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:29:18,984 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:29:18,990 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:29:18,995 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-500/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 23:29:23,544 >> Deleting older checkpoint [drive/MyDrive/out/gpt2/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 0.8022, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.53}\n",
            "{'loss': 0.7794, 'learning_rate': 1.377777777777778e-05, 'epoch': 0.62}\n",
            " 33% 750/2250 [06:11<10:47,  2.32it/s][INFO|trainer.py:710] 2023-02-13 23:31:10,573 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:31:10,575 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:31:10,576 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:31:10,576 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:08, 14.09it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  8.95it/s]\u001b[A\n",
            "  4% 5/116 [00:00<00:13,  8.45it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  8.13it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:14,  7.78it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:14,  7.60it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:14,  7.51it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.40it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.30it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.31it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.29it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:14,  7.21it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:14,  7.12it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:14,  7.14it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.09it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.10it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.16it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.19it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.16it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:13,  7.18it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:12,  7.21it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:12,  7.13it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.17it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.19it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.23it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.14it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.17it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:11,  7.17it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.17it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.11it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.10it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.10it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.12it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.16it/s]\u001b[A\n",
            " 32% 37/116 [00:05<00:11,  7.15it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:10,  7.21it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:10,  7.15it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.22it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.24it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.22it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.19it/s]\u001b[A\n",
            " 38% 44/116 [00:05<00:10,  7.13it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:09,  7.12it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.11it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.09it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.14it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.13it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.22it/s]\u001b[A\n",
            " 44% 51/116 [00:06<00:08,  7.25it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:08,  7.19it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.19it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.18it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.19it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.17it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.18it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:08,  7.24it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:07,  7.22it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.18it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.17it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.17it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.11it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.11it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.13it/s]\u001b[A\n",
            " 57% 66/116 [00:09<00:06,  7.16it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.28it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.24it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.21it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.20it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.15it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.18it/s]\u001b[A\n",
            " 63% 73/116 [00:10<00:05,  7.17it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.18it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.18it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.11it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.16it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  7.13it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  7.10it/s]\u001b[A\n",
            " 69% 80/116 [00:11<00:05,  7.15it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.12it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.14it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  7.10it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  7.14it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  7.11it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  7.09it/s]\u001b[A\n",
            " 75% 87/116 [00:12<00:04,  7.11it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.18it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.15it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.21it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.19it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  7.18it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  7.19it/s]\u001b[A\n",
            " 81% 94/116 [00:12<00:03,  7.12it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.08it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.07it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.09it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  7.10it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  7.16it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.23it/s]\u001b[A\n",
            " 87% 101/116 [00:13<00:02,  7.19it/s]\u001b[A\n",
            " 88% 102/116 [00:14<00:01,  7.17it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.17it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.17it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.15it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.18it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  7.23it/s]\u001b[A\n",
            " 93% 108/116 [00:14<00:01,  7.22it/s]\u001b[A\n",
            " 94% 109/116 [00:15<00:00,  7.18it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.17it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.17it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.21it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.31it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.29it/s]\u001b[A\n",
            " 99% 115/116 [00:15<00:00,  7.24it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8243730068206787, 'eval_accuracy': 0.593952476978302, 'eval_runtime': 16.1497, 'eval_samples_per_second': 57.338, 'eval_steps_per_second': 7.183, 'epoch': 0.67}\n",
            " 33% 750/2250 [06:27<10:47,  2.32it/s]\n",
            "100% 116/116 [00:16<00:00,  7.85it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:31:26,730 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-750\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:31:26,736 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-750/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:31:30,620 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:31:30,626 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:31:30,631 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-750/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 23:31:35,309 >> Deleting older checkpoint [drive/MyDrive/out/gpt2/checkpoint-1750] due to args.save_total_limit\n",
            "{'loss': 0.8077, 'learning_rate': 1.288888888888889e-05, 'epoch': 0.71}\n",
            "{'loss': 0.807, 'learning_rate': 1.2e-05, 'epoch': 0.8}\n",
            "{'loss': 0.7577, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.89}\n",
            " 44% 1000/2250 [08:22<08:57,  2.33it/s][INFO|trainer.py:710] 2023-02-13 23:33:22,203 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:33:22,205 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:33:22,205 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:33:22,206 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:07, 14.34it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  8.93it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  8.04it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:14,  7.71it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:14,  7.59it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:14,  7.46it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.37it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.41it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.33it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.23it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:14,  7.19it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:13,  7.26it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:13,  7.24it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.26it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.27it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.29it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.20it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.12it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:13,  7.16it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:13,  7.13it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:12,  7.14it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.15it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.28it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.23it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.17it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.15it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:12,  7.14it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.13it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.11it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.13it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.11it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.12it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.20it/s]\u001b[A\n",
            " 32% 37/116 [00:05<00:10,  7.25it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:10,  7.23it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:10,  7.23it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.18it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.15it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.12it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.09it/s]\u001b[A\n",
            " 38% 44/116 [00:06<00:10,  7.12it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:09,  7.13it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.15it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.20it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.18it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.20it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.16it/s]\u001b[A\n",
            " 44% 51/116 [00:06<00:09,  7.19it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:08,  7.13it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.11it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.12it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.11it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.14it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:08,  7.19it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:07,  7.21it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.17it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.15it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.14it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.15it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.10it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.09it/s]\u001b[A\n",
            " 57% 66/116 [00:09<00:07,  7.14it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.11it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.13it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.15it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.20it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.21it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.22it/s]\u001b[A\n",
            " 63% 73/116 [00:10<00:05,  7.20it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.21it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.21it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.23it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.21it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  7.23it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  7.28it/s]\u001b[A\n",
            " 69% 80/116 [00:11<00:04,  7.24it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.19it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.16it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  7.15it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  7.13it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  7.13it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  7.20it/s]\u001b[A\n",
            " 75% 87/116 [00:11<00:04,  7.18it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.19it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.25it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.20it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.06it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  6.78it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  6.92it/s]\u001b[A\n",
            " 81% 94/116 [00:13<00:03,  6.93it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.02it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.11it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.06it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  7.07it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  6.99it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.09it/s]\u001b[A\n",
            " 87% 101/116 [00:13<00:02,  7.16it/s]\u001b[A\n",
            " 88% 102/116 [00:14<00:01,  7.16it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.13it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.13it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.11it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.07it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  6.97it/s]\u001b[A\n",
            " 93% 108/116 [00:14<00:01,  7.06it/s]\u001b[A\n",
            " 94% 109/116 [00:15<00:00,  7.09it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.09it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.11it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.10it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.04it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.02it/s]\u001b[A\n",
            " 99% 115/116 [00:15<00:00,  7.07it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8127188682556152, 'eval_accuracy': 0.6317494511604309, 'eval_runtime': 16.2086, 'eval_samples_per_second': 57.13, 'eval_steps_per_second': 7.157, 'epoch': 0.89}\n",
            " 44% 1000/2250 [08:39<08:57,  2.33it/s]\n",
            "100% 116/116 [00:16<00:00,  7.64it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:33:38,418 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-1000\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:33:38,428 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:33:42,557 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:33:42,562 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:33:42,569 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 23:33:47,459 >> Deleting older checkpoint [drive/MyDrive/out/gpt2/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 0.7191, 'learning_rate': 1.0222222222222223e-05, 'epoch': 0.98}\n",
            "{'loss': 0.6688, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.07}\n",
            " 56% 1250/2250 [10:35<07:08,  2.33it/s][INFO|trainer.py:710] 2023-02-13 23:35:34,488 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:35:34,491 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:35:34,491 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:35:34,491 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:08, 14.03it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  8.87it/s]\u001b[A\n",
            "  4% 5/116 [00:00<00:13,  8.36it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  8.00it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:14,  7.66it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:14,  7.56it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:14,  7.43it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.37it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.26it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.29it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.29it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:14,  7.25it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:14,  7.18it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:13,  7.23it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.22it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.13it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.14it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.19it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.25it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:13,  7.18it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:12,  7.25it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:12,  7.29it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.18it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.20it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.23it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.22it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.19it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:11,  7.22it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.24it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.18it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.19it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.16it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.16it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.20it/s]\u001b[A\n",
            " 32% 37/116 [00:05<00:10,  7.25it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:10,  7.21it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:10,  7.23it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.22it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.17it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.19it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.15it/s]\u001b[A\n",
            " 38% 44/116 [00:05<00:09,  7.22it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:09,  7.24it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.23it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.20it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.18it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.14it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.12it/s]\u001b[A\n",
            " 44% 51/116 [00:06<00:09,  7.12it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.20it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.23it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.20it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.17it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.14it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:07,  7.14it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.14it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.17it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.17it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.21it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.26it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.23it/s]\u001b[A\n",
            " 57% 66/116 [00:09<00:06,  7.23it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.23it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.24it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.24it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.30it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.24it/s]\u001b[A\n",
            " 63% 73/116 [00:10<00:05,  7.24it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.23it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.23it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  7.25it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 69% 80/116 [00:10<00:04,  7.24it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.24it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  7.19it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  7.20it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  7.23it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  7.29it/s]\u001b[A\n",
            " 75% 87/116 [00:11<00:04,  7.24it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.21it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.18it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.19it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.17it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  7.12it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  7.16it/s]\u001b[A\n",
            " 81% 94/116 [00:12<00:03,  7.14it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.20it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.18it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.25it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  7.19it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  7.15it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.18it/s]\u001b[A\n",
            " 87% 101/116 [00:13<00:02,  7.17it/s]\u001b[A\n",
            " 88% 102/116 [00:14<00:01,  7.18it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.14it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.20it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.18it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.14it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  7.20it/s]\u001b[A\n",
            " 93% 108/116 [00:14<00:01,  7.00it/s]\u001b[A\n",
            " 94% 109/116 [00:15<00:00,  7.14it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.18it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.18it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.10it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.11it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.09it/s]\u001b[A\n",
            " 99% 115/116 [00:15<00:00,  7.07it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7978448271751404, 'eval_accuracy': 0.6447083950042725, 'eval_runtime': 16.1067, 'eval_samples_per_second': 57.492, 'eval_steps_per_second': 7.202, 'epoch': 1.11}\n",
            " 56% 1250/2250 [10:51<07:08,  2.33it/s]\n",
            "100% 116/116 [00:15<00:00,  7.68it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:35:50,602 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-1250\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:35:50,608 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-1250/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:35:54,607 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-1250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:35:54,612 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-1250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:35:54,619 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-1250/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 23:35:59,341 >> Deleting older checkpoint [drive/MyDrive/out/gpt2/checkpoint-2250] due to args.save_total_limit\n",
            "{'loss': 0.655, 'learning_rate': 8.444444444444446e-06, 'epoch': 1.16}\n",
            "{'loss': 0.6307, 'learning_rate': 7.555555555555556e-06, 'epoch': 1.24}\n",
            "{'loss': 0.6164, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.33}\n",
            " 67% 1500/2250 [12:47<05:20,  2.34it/s][INFO|trainer.py:710] 2023-02-13 23:37:46,308 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:37:46,311 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:37:46,311 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:37:46,311 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:08, 14.17it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  9.13it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  8.12it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:14,  7.76it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:14,  7.64it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:14,  7.46it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.32it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.28it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.19it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.13it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:14,  7.17it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:14,  7.19it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:13,  7.20it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.24it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.29it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.26it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.21it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.18it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:13,  7.10it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:12,  7.17it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:12,  7.15it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.21it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.18it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.16it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.13it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.14it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:12,  7.13it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.17it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.25it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.24it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.23it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.19it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.16it/s]\u001b[A\n",
            " 32% 37/116 [00:05<00:11,  7.13it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:10,  7.10it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:10,  7.13it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.19it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.25it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.21it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.17it/s]\u001b[A\n",
            " 38% 44/116 [00:06<00:10,  7.14it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:10,  7.09it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.10it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.11it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.13it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.16it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.19it/s]\u001b[A\n",
            " 44% 51/116 [00:06<00:09,  7.15it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.22it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.24it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.28it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:08,  7.23it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:07,  7.18it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.19it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.16it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.18it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.14it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.12it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.12it/s]\u001b[A\n",
            " 57% 66/116 [00:09<00:07,  7.11it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.16it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.22it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.22it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.21it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.21it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.17it/s]\u001b[A\n",
            " 63% 73/116 [00:10<00:05,  7.18it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.14it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.16it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.23it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.22it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  7.21it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  7.17it/s]\u001b[A\n",
            " 69% 80/116 [00:11<00:05,  7.16it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.24it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.25it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  7.22it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  7.21it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  7.20it/s]\u001b[A\n",
            " 75% 87/116 [00:11<00:04,  7.13it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.11it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.10it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.14it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.15it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  7.15it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  7.16it/s]\u001b[A\n",
            " 81% 94/116 [00:12<00:03,  7.19it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.16it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.16it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.17it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  7.18it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  7.17it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.13it/s]\u001b[A\n",
            " 87% 101/116 [00:13<00:02,  7.17it/s]\u001b[A\n",
            " 88% 102/116 [00:14<00:01,  7.17it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.17it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.17it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.20it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.19it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  7.17it/s]\u001b[A\n",
            " 93% 108/116 [00:14<00:01,  7.16it/s]\u001b[A\n",
            " 94% 109/116 [00:15<00:00,  7.05it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.07it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.09it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.18it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.20it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.17it/s]\u001b[A\n",
            " 99% 115/116 [00:15<00:00,  7.18it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8429656624794006, 'eval_accuracy': 0.6393088698387146, 'eval_runtime': 16.1476, 'eval_samples_per_second': 57.346, 'eval_steps_per_second': 7.184, 'epoch': 1.33}\n",
            " 67% 1500/2250 [13:03<05:20,  2.34it/s]\n",
            "100% 116/116 [00:16<00:00,  7.79it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:38:02,462 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-1500\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:38:02,468 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:38:06,622 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:38:06,630 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:38:06,634 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 23:38:11,267 >> Deleting older checkpoint [drive/MyDrive/out/gpt2/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 0.6336, 'learning_rate': 5.777777777777778e-06, 'epoch': 1.42}\n",
            "{'loss': 0.6266, 'learning_rate': 4.888888888888889e-06, 'epoch': 1.51}\n",
            " 78% 1750/2250 [15:00<03:33,  2.34it/s][INFO|trainer.py:710] 2023-02-13 23:39:59,649 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:39:59,652 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:39:59,652 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:39:59,652 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:07, 14.58it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  9.04it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  8.01it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:13,  7.83it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:14,  7.66it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:14,  7.54it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.51it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.41it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.42it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.29it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:14,  7.24it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:13,  7.22it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:13,  7.28it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.30it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.32it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.29it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.17it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.16it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:13,  7.14it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:13,  7.13it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:12,  7.17it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.14it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.17it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.13it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.13it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.17it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:11,  7.19it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.24it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.23it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.16it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.13it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.08it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.08it/s]\u001b[A\n",
            " 32% 37/116 [00:05<00:11,  7.08it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:10,  7.17it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:10,  7.18it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.15it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.19it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.17it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.12it/s]\u001b[A\n",
            " 38% 44/116 [00:05<00:10,  7.12it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:09,  7.14it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.09it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.09it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.06it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.08it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.07it/s]\u001b[A\n",
            " 44% 51/116 [00:06<00:09,  7.08it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:09,  7.09it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.04it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.04it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.05it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.08it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.09it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:08,  7.07it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:08,  7.05it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.06it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.11it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.13it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.23it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.16it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.20it/s]\u001b[A\n",
            " 57% 66/116 [00:09<00:06,  7.14it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.14it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.09it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.01it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.11it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.08it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.10it/s]\u001b[A\n",
            " 63% 73/116 [00:10<00:06,  7.06it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.07it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.06it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.04it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.09it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  7.09it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  7.10it/s]\u001b[A\n",
            " 69% 80/116 [00:11<00:05,  7.14it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.17it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.16it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  7.14it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  7.19it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  7.19it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  7.19it/s]\u001b[A\n",
            " 75% 87/116 [00:12<00:04,  7.14it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.12it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.11it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.05it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.09it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  7.08it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  7.09it/s]\u001b[A\n",
            " 81% 94/116 [00:13<00:03,  7.09it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.11it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.13it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.12it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  7.20it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  7.24it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.20it/s]\u001b[A\n",
            " 87% 101/116 [00:14<00:02,  7.20it/s]\u001b[A\n",
            " 88% 102/116 [00:14<00:01,  7.19it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.14it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.09it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.09it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.14it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  7.20it/s]\u001b[A\n",
            " 93% 108/116 [00:14<00:01,  7.26it/s]\u001b[A\n",
            " 94% 109/116 [00:15<00:00,  7.25it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.17it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.14it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.14it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.16it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.16it/s]\u001b[A\n",
            " 99% 115/116 [00:15<00:00,  7.21it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8323885202407837, 'eval_accuracy': 0.6544276475906372, 'eval_runtime': 16.2065, 'eval_samples_per_second': 57.138, 'eval_steps_per_second': 7.158, 'epoch': 1.56}\n",
            " 78% 1750/2250 [15:16<03:33,  2.34it/s]\n",
            "100% 116/116 [00:16<00:00,  7.85it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:40:15,862 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-1750\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:40:15,869 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-1750/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:40:19,704 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-1750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:40:20,318 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-1750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:40:20,324 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-1750/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 23:40:24,802 >> Deleting older checkpoint [drive/MyDrive/out/gpt2/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.6135, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.6}\n",
            "{'loss': 0.6321, 'learning_rate': 3.1111111111111116e-06, 'epoch': 1.69}\n",
            "{'loss': 0.6239, 'learning_rate': 2.222222222222222e-06, 'epoch': 1.78}\n",
            " 89% 2000/2250 [17:12<01:47,  2.33it/s][INFO|trainer.py:710] 2023-02-13 23:42:12,044 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:42:12,047 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:42:12,047 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:42:12,047 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:08, 14.12it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  8.94it/s]\u001b[A\n",
            "  4% 5/116 [00:00<00:13,  8.34it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  7.93it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:14,  7.60it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:14,  7.51it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:14,  7.41it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.27it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.33it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.31it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.19it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:14,  7.18it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:14,  7.17it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:13,  7.16it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.21it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.18it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.19it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.18it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.18it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:13,  7.17it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:13,  7.12it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:13,  6.99it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.07it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.12it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.13it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.20it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.19it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:11,  7.17it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.15it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.15it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.11it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.18it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.16it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.13it/s]\u001b[A\n",
            " 32% 37/116 [00:05<00:11,  7.09it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:11,  7.00it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:11,  6.97it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.02it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.12it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.20it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.20it/s]\u001b[A\n",
            " 38% 44/116 [00:06<00:10,  7.16it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:10,  7.10it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.04it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.08it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.19it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.22it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.25it/s]\u001b[A\n",
            " 44% 51/116 [00:07<00:09,  7.13it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.02it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.08it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.14it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.20it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.21it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:08,  7.14it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:08,  7.12it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.13it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.08it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.12it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.13it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.18it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.06it/s]\u001b[A\n",
            " 57% 66/116 [00:09<00:07,  7.08it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.14it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.13it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.15it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.20it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.15it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.10it/s]\u001b[A\n",
            " 63% 73/116 [00:10<00:06,  7.03it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.08it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.04it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.08it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.01it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  6.63it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  6.77it/s]\u001b[A\n",
            " 69% 80/116 [00:11<00:05,  6.91it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.03it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.06it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  7.09it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  6.85it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  6.93it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  7.05it/s]\u001b[A\n",
            " 75% 87/116 [00:12<00:04,  7.15it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.14it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.22it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.23it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.06it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  7.00it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  7.06it/s]\u001b[A\n",
            " 81% 94/116 [00:13<00:03,  7.17it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.20it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.33it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.33it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  7.22it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  7.19it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.26it/s]\u001b[A\n",
            " 87% 101/116 [00:14<00:02,  7.33it/s]\u001b[A\n",
            " 88% 102/116 [00:14<00:01,  7.36it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.37it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.27it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.18it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.12it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  7.12it/s]\u001b[A\n",
            " 93% 108/116 [00:15<00:01,  7.14it/s]\u001b[A\n",
            " 94% 109/116 [00:15<00:00,  7.18it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.25it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.26it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.19it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.21it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.23it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7809873223304749, 'eval_accuracy': 0.6652267575263977, 'eval_runtime': 16.2277, 'eval_samples_per_second': 57.063, 'eval_steps_per_second': 7.148, 'epoch': 1.78}\n",
            " 89% 2000/2250 [17:29<01:47,  2.33it/s]\n",
            "100% 116/116 [00:16<00:00,  7.21it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:42:28,279 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-2000\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:42:28,284 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:42:32,324 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:42:32,329 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:42:32,829 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 23:42:36,855 >> Deleting older checkpoint [drive/MyDrive/out/gpt2/checkpoint-750] due to args.save_total_limit\n",
            "{'loss': 0.5954, 'learning_rate': 1.3333333333333334e-06, 'epoch': 1.87}\n",
            "{'loss': 0.6386, 'learning_rate': 4.444444444444445e-07, 'epoch': 1.96}\n",
            "100% 2250/2250 [19:24<00:00,  2.34it/s][INFO|trainer.py:710] 2023-02-13 23:44:23,728 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:44:23,730 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:44:23,730 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:44:23,730 >>   Batch size = 8\n",
            "\n",
            "  0% 0/116 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/116 [00:00<00:08, 14.21it/s]\u001b[A\n",
            "  3% 4/116 [00:00<00:12,  9.08it/s]\u001b[A\n",
            "  5% 6/116 [00:00<00:13,  8.18it/s]\u001b[A\n",
            "  6% 7/116 [00:00<00:13,  7.85it/s]\u001b[A\n",
            "  7% 8/116 [00:00<00:14,  7.70it/s]\u001b[A\n",
            "  8% 9/116 [00:01<00:14,  7.62it/s]\u001b[A\n",
            "  9% 10/116 [00:01<00:14,  7.44it/s]\u001b[A\n",
            "  9% 11/116 [00:01<00:14,  7.39it/s]\u001b[A\n",
            " 10% 12/116 [00:01<00:14,  7.32it/s]\u001b[A\n",
            " 11% 13/116 [00:01<00:14,  7.29it/s]\u001b[A\n",
            " 12% 14/116 [00:01<00:14,  7.23it/s]\u001b[A\n",
            " 13% 15/116 [00:01<00:14,  7.21it/s]\u001b[A\n",
            " 14% 16/116 [00:02<00:13,  7.21it/s]\u001b[A\n",
            " 15% 17/116 [00:02<00:13,  7.33it/s]\u001b[A\n",
            " 16% 18/116 [00:02<00:13,  7.25it/s]\u001b[A\n",
            " 16% 19/116 [00:02<00:13,  7.23it/s]\u001b[A\n",
            " 17% 20/116 [00:02<00:13,  7.29it/s]\u001b[A\n",
            " 18% 21/116 [00:02<00:13,  7.25it/s]\u001b[A\n",
            " 19% 22/116 [00:02<00:12,  7.25it/s]\u001b[A\n",
            " 20% 23/116 [00:03<00:12,  7.20it/s]\u001b[A\n",
            " 21% 24/116 [00:03<00:12,  7.23it/s]\u001b[A\n",
            " 22% 25/116 [00:03<00:12,  7.19it/s]\u001b[A\n",
            " 22% 26/116 [00:03<00:12,  7.16it/s]\u001b[A\n",
            " 23% 27/116 [00:03<00:12,  7.14it/s]\u001b[A\n",
            " 24% 28/116 [00:03<00:12,  7.08it/s]\u001b[A\n",
            " 25% 29/116 [00:03<00:12,  7.09it/s]\u001b[A\n",
            " 26% 30/116 [00:04<00:12,  7.10it/s]\u001b[A\n",
            " 27% 31/116 [00:04<00:11,  7.14it/s]\u001b[A\n",
            " 28% 32/116 [00:04<00:11,  7.27it/s]\u001b[A\n",
            " 28% 33/116 [00:04<00:11,  7.25it/s]\u001b[A\n",
            " 29% 34/116 [00:04<00:11,  7.21it/s]\u001b[A\n",
            " 30% 35/116 [00:04<00:11,  7.17it/s]\u001b[A\n",
            " 31% 36/116 [00:04<00:11,  7.12it/s]\u001b[A\n",
            " 32% 37/116 [00:05<00:11,  7.14it/s]\u001b[A\n",
            " 33% 38/116 [00:05<00:10,  7.13it/s]\u001b[A\n",
            " 34% 39/116 [00:05<00:10,  7.21it/s]\u001b[A\n",
            " 34% 40/116 [00:05<00:10,  7.23it/s]\u001b[A\n",
            " 35% 41/116 [00:05<00:10,  7.22it/s]\u001b[A\n",
            " 36% 42/116 [00:05<00:10,  7.20it/s]\u001b[A\n",
            " 37% 43/116 [00:05<00:10,  7.12it/s]\u001b[A\n",
            " 38% 44/116 [00:05<00:10,  7.10it/s]\u001b[A\n",
            " 39% 45/116 [00:06<00:10,  7.10it/s]\u001b[A\n",
            " 40% 46/116 [00:06<00:09,  7.10it/s]\u001b[A\n",
            " 41% 47/116 [00:06<00:09,  7.16it/s]\u001b[A\n",
            " 41% 48/116 [00:06<00:09,  7.22it/s]\u001b[A\n",
            " 42% 49/116 [00:06<00:09,  7.21it/s]\u001b[A\n",
            " 43% 50/116 [00:06<00:09,  7.18it/s]\u001b[A\n",
            " 44% 51/116 [00:06<00:09,  7.14it/s]\u001b[A\n",
            " 45% 52/116 [00:07<00:08,  7.12it/s]\u001b[A\n",
            " 46% 53/116 [00:07<00:08,  7.14it/s]\u001b[A\n",
            " 47% 54/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 47% 55/116 [00:07<00:08,  7.15it/s]\u001b[A\n",
            " 48% 56/116 [00:07<00:08,  7.16it/s]\u001b[A\n",
            " 49% 57/116 [00:07<00:08,  7.10it/s]\u001b[A\n",
            " 50% 58/116 [00:07<00:08,  7.12it/s]\u001b[A\n",
            " 51% 59/116 [00:08<00:07,  7.13it/s]\u001b[A\n",
            " 52% 60/116 [00:08<00:07,  7.21it/s]\u001b[A\n",
            " 53% 61/116 [00:08<00:07,  7.19it/s]\u001b[A\n",
            " 53% 62/116 [00:08<00:07,  7.18it/s]\u001b[A\n",
            " 54% 63/116 [00:08<00:07,  7.19it/s]\u001b[A\n",
            " 55% 64/116 [00:08<00:07,  7.13it/s]\u001b[A\n",
            " 56% 65/116 [00:08<00:07,  7.14it/s]\u001b[A\n",
            " 57% 66/116 [00:09<00:07,  7.12it/s]\u001b[A\n",
            " 58% 67/116 [00:09<00:06,  7.12it/s]\u001b[A\n",
            " 59% 68/116 [00:09<00:06,  7.08it/s]\u001b[A\n",
            " 59% 69/116 [00:09<00:06,  7.05it/s]\u001b[A\n",
            " 60% 70/116 [00:09<00:06,  7.02it/s]\u001b[A\n",
            " 61% 71/116 [00:09<00:06,  7.04it/s]\u001b[A\n",
            " 62% 72/116 [00:09<00:06,  7.08it/s]\u001b[A\n",
            " 63% 73/116 [00:10<00:06,  7.08it/s]\u001b[A\n",
            " 64% 74/116 [00:10<00:05,  7.07it/s]\u001b[A\n",
            " 65% 75/116 [00:10<00:05,  7.08it/s]\u001b[A\n",
            " 66% 76/116 [00:10<00:05,  7.13it/s]\u001b[A\n",
            " 66% 77/116 [00:10<00:05,  7.09it/s]\u001b[A\n",
            " 67% 78/116 [00:10<00:05,  7.20it/s]\u001b[A\n",
            " 68% 79/116 [00:10<00:05,  7.23it/s]\u001b[A\n",
            " 69% 80/116 [00:11<00:04,  7.22it/s]\u001b[A\n",
            " 70% 81/116 [00:11<00:04,  7.13it/s]\u001b[A\n",
            " 71% 82/116 [00:11<00:04,  7.05it/s]\u001b[A\n",
            " 72% 83/116 [00:11<00:04,  6.93it/s]\u001b[A\n",
            " 72% 84/116 [00:11<00:04,  6.91it/s]\u001b[A\n",
            " 73% 85/116 [00:11<00:04,  7.02it/s]\u001b[A\n",
            " 74% 86/116 [00:11<00:04,  6.94it/s]\u001b[A\n",
            " 75% 87/116 [00:12<00:04,  7.10it/s]\u001b[A\n",
            " 76% 88/116 [00:12<00:03,  7.13it/s]\u001b[A\n",
            " 77% 89/116 [00:12<00:03,  7.09it/s]\u001b[A\n",
            " 78% 90/116 [00:12<00:03,  7.10it/s]\u001b[A\n",
            " 78% 91/116 [00:12<00:03,  7.05it/s]\u001b[A\n",
            " 79% 92/116 [00:12<00:03,  7.06it/s]\u001b[A\n",
            " 80% 93/116 [00:12<00:03,  7.00it/s]\u001b[A\n",
            " 81% 94/116 [00:13<00:03,  7.09it/s]\u001b[A\n",
            " 82% 95/116 [00:13<00:02,  7.13it/s]\u001b[A\n",
            " 83% 96/116 [00:13<00:02,  7.00it/s]\u001b[A\n",
            " 84% 97/116 [00:13<00:02,  7.02it/s]\u001b[A\n",
            " 84% 98/116 [00:13<00:02,  6.99it/s]\u001b[A\n",
            " 85% 99/116 [00:13<00:02,  6.96it/s]\u001b[A\n",
            " 86% 100/116 [00:13<00:02,  7.01it/s]\u001b[A\n",
            " 87% 101/116 [00:14<00:02,  7.04it/s]\u001b[A\n",
            " 88% 102/116 [00:14<00:01,  7.11it/s]\u001b[A\n",
            " 89% 103/116 [00:14<00:01,  7.04it/s]\u001b[A\n",
            " 90% 104/116 [00:14<00:01,  7.06it/s]\u001b[A\n",
            " 91% 105/116 [00:14<00:01,  7.04it/s]\u001b[A\n",
            " 91% 106/116 [00:14<00:01,  7.07it/s]\u001b[A\n",
            " 92% 107/116 [00:14<00:01,  7.12it/s]\u001b[A\n",
            " 93% 108/116 [00:14<00:01,  7.18it/s]\u001b[A\n",
            " 94% 109/116 [00:15<00:00,  7.23it/s]\u001b[A\n",
            " 95% 110/116 [00:15<00:00,  7.25it/s]\u001b[A\n",
            " 96% 111/116 [00:15<00:00,  7.22it/s]\u001b[A\n",
            " 97% 112/116 [00:15<00:00,  7.14it/s]\u001b[A\n",
            " 97% 113/116 [00:15<00:00,  7.12it/s]\u001b[A\n",
            " 98% 114/116 [00:15<00:00,  7.10it/s]\u001b[A\n",
            " 99% 115/116 [00:15<00:00,  7.12it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7831594944000244, 'eval_accuracy': 0.6706263422966003, 'eval_runtime': 16.2299, 'eval_samples_per_second': 57.055, 'eval_steps_per_second': 7.147, 'epoch': 2.0}\n",
            "100% 2250/2250 [19:40<00:00,  2.34it/s]\n",
            "100% 116/116 [00:16<00:00,  7.74it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-13 23:44:39,967 >> Saving model checkpoint to drive/MyDrive/out/gpt2/checkpoint-2250\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:44:39,973 >> Configuration saved in drive/MyDrive/out/gpt2/checkpoint-2250/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:44:44,152 >> Model weights saved in drive/MyDrive/out/gpt2/checkpoint-2250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:44:44,172 >> tokenizer config file saved in drive/MyDrive/out/gpt2/checkpoint-2250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:44:44,181 >> Special tokens file saved in drive/MyDrive/out/gpt2/checkpoint-2250/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 23:44:48,932 >> Deleting older checkpoint [drive/MyDrive/out/gpt2/checkpoint-1000] due to args.save_total_limit\n",
            "[INFO|trainer.py:1901] 2023-02-13 23:44:49,077 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2025] 2023-02-13 23:44:49,077 >> Loading best model from drive/MyDrive/out/gpt2/checkpoint-2250 (score: 0.6706263422966003).\n",
            "{'train_runtime': 1193.1093, 'train_samples_per_second': 15.087, 'train_steps_per_second': 1.886, 'train_loss': 0.7229580230712891, 'epoch': 2.0}\n",
            "100% 2250/2250 [19:53<00:00,  1.89it/s]\n",
            "[INFO|trainer.py:2709] 2023-02-13 23:44:52,326 >> Saving model checkpoint to drive/MyDrive/out/gpt2\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 23:44:52,335 >> Configuration saved in drive/MyDrive/out/gpt2/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 23:44:56,511 >> Model weights saved in drive/MyDrive/out/gpt2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 23:44:56,520 >> tokenizer config file saved in drive/MyDrive/out/gpt2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 23:44:56,525 >> Special tokens file saved in drive/MyDrive/out/gpt2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        2.0\n",
            "  train_loss               =      0.723\n",
            "  train_runtime            = 0:19:53.10\n",
            "  train_samples            =       9000\n",
            "  train_samples_per_second =     15.087\n",
            "  train_steps_per_second   =      1.886\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:710] 2023-02-13 23:44:57,154 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:44:57,155 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:44:57,155 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:44:57,155 >>   Batch size = 8\n",
            "100% 116/116 [00:15<00:00,  7.42it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        2.0\n",
            "  eval_accuracy           =     0.6706\n",
            "  eval_loss               =     0.7832\n",
            "  eval_runtime            = 0:00:15.79\n",
            "  eval_samples            =        926\n",
            "  eval_samples_per_second =     58.629\n",
            "  eval_steps_per_second   =      7.344\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:710] 2023-02-13 23:45:12,976 >> The following columns in the test set don't have a corresponding argument in `GPT2CustomClassifier.forward` and have been ignored: text. If text are not expected by `GPT2CustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 23:45:12,977 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 23:45:12,977 >>   Num examples = 27742\n",
            "[INFO|trainer.py:2969] 2023-02-13 23:45:12,977 >>   Batch size = 8\n",
            "100% 3468/3468 [08:05<00:00,  7.14it/s]\n",
            "INFO:__main__:***** Predict results None *****\n",
            "[INFO|modelcard.py:449] 2023-02-13 23:53:19,832 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6706263422966003}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mdydXkKnmZer"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}