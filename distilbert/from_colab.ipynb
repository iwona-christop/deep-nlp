{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_ZcNzCvfYjNH",
        "EI-art_RYlvt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "_ZcNzCvfYjNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets evaluate transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipkZoqGZYfzS",
        "outputId": "e5a113e5-3f0c-4d29-e7f9-0c584ddd19a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tokenizers, xxhash, urllib3, multiprocess, responses, huggingface-hub, transformers, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 evaluate-0.4.0 huggingface-hub-0.12.0 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.2 transformers-4.26.1 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `wget`"
      ],
      "metadata": {
        "id": "EI-art_RYlvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/distilbert/model.py' -O 'distilbert.py'\n",
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/distilbert/data/test.json'\n",
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/distilbert/data/train.json'\n",
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/distilbert/data/valid.json'\n",
        "! wget 'https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/run_glue.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWBOYIGmbSXX",
        "outputId": "cc8c6956-c74a-45dc-e800-217f85509f31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-13 20:51:08--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/distilbert/model.py\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3522 (3.4K) [text/plain]\n",
            "Saving to: ‘distilbert.py’\n",
            "\n",
            "distilbert.py       100%[===================>]   3.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-13 20:51:10 (638 MB/s) - ‘distilbert.py’ saved [3522/3522]\n",
            "\n",
            "--2023-02-13 20:51:10--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/distilbert/data/test.json\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19695881 (19M) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>]  18.78M  3.62MB/s    in 6.7s    \n",
            "\n",
            "2023-02-13 20:51:18 (2.80 MB/s) - ‘test.json’ saved [19695881/19695881]\n",
            "\n",
            "--2023-02-13 20:51:18--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/distilbert/data/train.json\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6385138 (6.1M) [text/plain]\n",
            "Saving to: ‘train.json’\n",
            "\n",
            "train.json          100%[===================>]   6.09M  1.41MB/s    in 4.3s    \n",
            "\n",
            "2023-02-13 20:51:23 (1.41 MB/s) - ‘train.json’ saved [6385138/6385138]\n",
            "\n",
            "--2023-02-13 20:51:23--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/distilbert/data/valid.json\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 896854 (876K) [text/plain]\n",
            "Saving to: ‘valid.json’\n",
            "\n",
            "valid.json          100%[===================>] 875.83K   689KB/s    in 1.3s    \n",
            "\n",
            "2023-02-13 20:51:25 (689 KB/s) - ‘valid.json’ saved [896854/896854]\n",
            "\n",
            "--2023-02-13 20:51:25--  https://git.wmi.amu.edu.pl/s443930/deep-nlp/raw/branch/master/run_glue.py\n",
            "Resolving git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)... 150.254.78.40\n",
            "Connecting to git.wmi.amu.edu.pl (git.wmi.amu.edu.pl)|150.254.78.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30419 (30K) [text/plain]\n",
            "Saving to: ‘run_glue.py’\n",
            "\n",
            "run_glue.py         100%[===================>]  29.71K   140KB/s    in 0.2s    \n",
            "\n",
            "2023-02-13 20:51:26 (140 KB/s) - ‘run_glue.py’ saved [30419/30419]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `run_glue.py`"
      ],
      "metadata": {
        "id": "eWi22LUKYrD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python run_glue.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path distilbert-base-cased \\\n",
        "  --custom_model distilbert_custom \\\n",
        "  --train_file train.json  \\\n",
        "  --validation_file valid.json \\\n",
        "  --test_file test.json \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --per_device_eval_batch_size 32 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --max_seq_length 128 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --max_eval_samples 2000 \\\n",
        "  --max_steps 2500 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 250 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 100 \\\n",
        "  --eval_steps 250 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model accuracy \\\n",
        "  --greater_is_better True \\\n",
        "  --load_best_model_at_end True \\\n",
        "  --output_dir out/distilbert \\\n",
        "  --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3dDFy9iX6zc",
        "outputId": "6da88e6c-fb19-41e5-b358-1500451c46d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-13 20:52:02.976281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-13 20:52:03.823408: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-13 20:52:03.823546: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-13 20:52:03.823566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=250,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/distilbert/runs/Feb13_20-52-06_4f293668aee7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=2500,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=out/distilbert,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=32,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/distilbert,\n",
            "save_on_each_node=False,\n",
            "save_steps=250,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: train.json\n",
            "INFO:__main__:load a local file for validation: valid.json\n",
            "INFO:__main__:load a local file for test: test.json\n",
            "WARNING:datasets.builder:Using custom data configuration default-c46d179e7b217d77\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Generating dataset json (/content/.cache_training/json/default-c46d179e7b217d77/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "Downloading and preparing dataset json/default to /content/.cache_training/json/default-c46d179e7b217d77/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 9701.55it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1398.41it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /content/.cache_training/json/default-c46d179e7b217d77/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 490.96it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 411/411 [00:00<00:00, 69.1kB/s]\n",
            "[INFO|configuration_utils.py:660] 2023-02-13 20:52:11,685 >> loading configuration file config.json from cache at .cache_training/models--distilbert-base-cased/snapshots/9d7568e4b20ed5db15ee30e99c7219bde9990762/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-13 20:52:11,686 >> Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-cased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 11.2kB/s]\n",
            "[INFO|configuration_utils.py:660] 2023-02-13 20:52:13,500 >> loading configuration file config.json from cache at .cache_training/models--distilbert-base-cased/snapshots/9d7568e4b20ed5db15ee30e99c7219bde9990762/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-13 20:52:13,500 >> Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-cased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Downloading (…)solve/main/vocab.txt: 100% 213k/213k [00:00<00:00, 239kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 436k/436k [00:00<00:00, 491kB/s]\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 20:52:20,736 >> loading file vocab.txt from cache at .cache_training/models--distilbert-base-cased/snapshots/9d7568e4b20ed5db15ee30e99c7219bde9990762/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 20:52:20,736 >> loading file tokenizer.json from cache at .cache_training/models--distilbert-base-cased/snapshots/9d7568e4b20ed5db15ee30e99c7219bde9990762/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 20:52:20,736 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 20:52:20,736 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-13 20:52:20,736 >> loading file tokenizer_config.json from cache at .cache_training/models--distilbert-base-cased/snapshots/9d7568e4b20ed5db15ee30e99c7219bde9990762/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:660] 2023-02-13 20:52:20,736 >> loading configuration file config.json from cache at .cache_training/models--distilbert-base-cased/snapshots/9d7568e4b20ed5db15ee30e99c7219bde9990762/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-13 20:52:20,737 >> Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-cased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "INFO:__main__:Using hidden states in model: False\n",
            "INFO:__main__:Using implementation from class: DistilbertCustomClassifier\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 263M/263M [00:02<00:00, 96.8MB/s]\n",
            "[INFO|modeling_utils.py:2275] 2023-02-13 20:52:24,427 >> loading weights file pytorch_model.bin from cache at .cache_training/models--distilbert-base-cased/snapshots/9d7568e4b20ed5db15ee30e99c7219bde9990762/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2847] 2023-02-13 20:52:25,728 >> Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilbertCustomClassifier: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilbertCustomClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilbertCustomClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2859] 2023-02-13 20:52:25,728 >> Some weights of DistilbertCustomClassifier were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'linear.bias', 'classifier.weight', 'out_proj.bias', 'linear.weight', 'pre_classifier.bias', 'out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-c46d179e7b217d77/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-341c7208fda0ebcd.arrow\n",
            "Running tokenizer on dataset: 100% 9/9 [00:03<00:00,  2.75ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-c46d179e7b217d77/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-80d31f3cd5f43922.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  4.37ba/s]\n",
            "Running tokenizer on dataset:   0% 0/28 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-c46d179e7b217d77/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-bcb377213fd4bf47.arrow\n",
            "Running tokenizer on dataset: 100% 28/28 [00:11<00:00,  2.36ba/s]\n",
            "INFO:__main__:Set 420 samples for 2-class\n",
            "INFO:__main__:Set 420 samples for 1-class\n",
            "INFO:__main__:Set 420 samples for 0-class\n",
            "INFO:__main__:Sample 1824 of the training set: {'label': 1, 'text': \"Poured from a bottle into my pewter Jagerstein at refrigetor chilled temperature. I peaked a look at the color from my friends glass and found it to be somewhere between amber and stout/porter black. The kind of color that would decieve the average drinker into thinking that this was going to be a beer as thick as motor oil. But that's not how they roll in VT. The smell betrayed a sweeter character, almost over emphasizing the malt properties. The taste was a bit on the hoppy side. That is to say that, the aftertaste left you remembering more of the hops than the also very strong, caramel malt taste. The mouthfeel was perfect for a cool summer night. This would probably be a great beer from late April to early June and late September to early November. I could drink a few of these at a time, but would probably want to move on to something lighter after that.\", 'input_ids': [101, 18959, 10105, 1121, 170, 5346, 1154, 1139, 185, 5773, 2083, 147, 12344, 7879, 1120, 1231, 2087, 17305, 20713, 1197, 27278, 4143, 119, 146, 6009, 170, 1440, 1120, 1103, 2942, 1121, 1139, 2053, 2525, 1105, 1276, 1122, 1106, 1129, 4476, 1206, 18376, 1105, 188, 2430, 3818, 120, 4104, 1200, 1602, 119, 1109, 1912, 1104, 2942, 1115, 1156, 1260, 25982, 2707, 1103, 1903, 3668, 1200, 1154, 2422, 1115, 1142, 1108, 1280, 1106, 1129, 170, 5298, 1112, 3528, 1112, 5968, 2949, 119, 1252, 1115, 112, 188, 1136, 1293, 1152, 5155, 1107, 159, 1942, 119, 1109, 4773, 12546, 170, 4105, 1200, 1959, 117, 1593, 1166, 25841, 1103, 12477, 6066, 4625, 119, 1109, 5080, 1108, 170, 2113, 1113, 1103, 6974, 5005, 1334, 119, 1337, 1110, 1106, 1474, 1115, 117, 1103, 1170, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "INFO:__main__:Sample 409 of the training set: {'label': 1, 'text': 'This beer isn\\'t bad, but there really isn\\'t anything I can say to strongly recommend it; there aren\\'t any really distinctive or unique flavors and I thought I detected a faint flavor defect or two. Organic Revolution doesn\\'t do anything to change my impression that in general the quality (or at least degree of \"interesting\") organic beers remains distinctly below that of non-organic beers. Pours a thick gold lager-color with a very tacky white head that is loosely packed into the top of my glass. Appearance of the beer\\'s body is a bit disconcerting - it has an oily-swirlyness to it. Aroma is only slightly hoppy, with some fruity vinous-ness. Flavor is balanced, but a bit dull with a very faint hoppiness. Some vinous-ness and diacetyl are evident to me.', 'input_ids': [101, 1188, 5298, 2762, 112, 189, 2213, 117, 1133, 1175, 1541, 2762, 112, 189, 1625, 146, 1169, 1474, 1106, 5473, 18029, 1122, 132, 1175, 4597, 112, 189, 1251, 1541, 7884, 1137, 3527, 16852, 1116, 1105, 146, 1354, 146, 11168, 170, 7859, 16852, 23912, 1137, 1160, 119, 27977, 4543, 2144, 112, 189, 1202, 1625, 1106, 1849, 1139, 8351, 1115, 1107, 1704, 1103, 3068, 113, 1137, 1120, 1655, 2178, 1104, 107, 5426, 107, 114, 7878, 23147, 2606, 21389, 2071, 1115, 1104, 1664, 118, 7878, 23147, 119, 18959, 7719, 170, 3528, 2284, 2495, 2895, 118, 2942, 1114, 170, 1304, 27629, 23143, 1653, 1246, 1115, 1110, 12158, 8733, 1154, 1103, 1499, 1104, 1139, 2525, 119, 27717, 1104, 1103, 5298, 112, 188, 1404, 1110, 170, 2113, 19959, 3633, 21811, 118, 1122, 1144, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "INFO:__main__:Sample 4506 of the training set: {'label': 2, 'text': \"On tap at the brewery on 6/28/09. Pours solid black with mocha head, great lacing. Aroma is certainly dominated by chocolate, but gives way to smoke and coffee. This is a straight up sweet, smooth, balanced stout with a great chocolate finish, and for the style, it's incredible. Will be looking for this on the east coast.\", 'input_ids': [101, 1212, 12999, 1120, 1103, 17876, 1113, 127, 120, 1743, 120, 4925, 119, 18959, 7719, 4600, 1602, 1114, 182, 9962, 1161, 1246, 117, 1632, 2495, 4869, 119, 138, 18885, 1110, 4664, 6226, 1118, 8888, 117, 1133, 3114, 1236, 1106, 5427, 1105, 3538, 119, 1188, 1110, 170, 2632, 1146, 4105, 117, 5307, 117, 12591, 188, 2430, 3818, 1114, 170, 1632, 8888, 3146, 117, 1105, 1111, 1103, 1947, 117, 1122, 112, 188, 10965, 119, 3100, 1129, 1702, 1111, 1142, 1113, 1103, 1746, 3153, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 3.18MB/s]\n",
            "[INFO|trainer.py:511] 2023-02-13 20:52:48,375 >> max_steps is given, it will override any value given in num_train_epochs\n",
            "[INFO|trainer.py:710] 2023-02-13 20:52:48,376 >> The following columns in the training set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1650] 2023-02-13 20:52:48,384 >> ***** Running training *****\n",
            "[INFO|trainer.py:1651] 2023-02-13 20:52:48,384 >>   Num examples = 9000\n",
            "[INFO|trainer.py:1652] 2023-02-13 20:52:48,384 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1653] 2023-02-13 20:52:48,384 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1654] 2023-02-13 20:52:48,384 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1655] 2023-02-13 20:52:48,384 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1656] 2023-02-13 20:52:48,384 >>   Total optimization steps = 2500\n",
            "[INFO|trainer.py:1657] 2023-02-13 20:52:48,385 >>   Number of trainable parameters = 66376710\n",
            "{'loss': 0.891, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.35}\n",
            "{'loss': 0.772, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.71}\n",
            " 10% 250/2500 [01:21<12:11,  3.08it/s][INFO|trainer.py:710] 2023-02-13 20:54:09,595 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 20:54:09,597 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 20:54:09,597 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 20:54:09,597 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 18.93it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.61it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.32it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02,  9.81it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:01,  9.56it/s]\u001b[A\n",
            " 38% 11/29 [00:01<00:01,  9.47it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.40it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.31it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.26it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.22it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.20it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.30it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.34it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.30it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.34it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.40it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.33it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.40it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.36it/s]\u001b[A\n",
            "{'eval_loss': 0.7741678357124329, 'eval_accuracy': 0.6436285376548767, 'eval_runtime': 3.1291, 'eval_samples_per_second': 295.935, 'eval_steps_per_second': 9.268, 'epoch': 0.89}\n",
            "\n",
            " 10% 250/2500 [01:24<12:11,  3.08it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 20:54:12,727 >> Saving model checkpoint to out/distilbert/checkpoint-250\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 20:54:12,728 >> Configuration saved in out/distilbert/checkpoint-250/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 20:54:13,228 >> Model weights saved in out/distilbert/checkpoint-250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 20:54:13,229 >> tokenizer config file saved in out/distilbert/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 20:54:13,229 >> Special tokens file saved in out/distilbert/checkpoint-250/special_tokens_map.json\n",
            "{'loss': 0.7307, 'learning_rate': 1.76e-05, 'epoch': 1.06}\n",
            "{'loss': 0.6643, 'learning_rate': 1.6800000000000002e-05, 'epoch': 1.42}\n",
            "{'loss': 0.6519, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.77}\n",
            " 20% 500/2500 [02:48<10:48,  3.08it/s][INFO|trainer.py:710] 2023-02-13 20:55:36,895 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 20:55:36,897 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 20:55:36,897 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 20:55:36,897 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 19.03it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.75it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.52it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02, 10.03it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:01,  9.80it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.57it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.47it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.48it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.49it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.45it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.41it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.42it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.36it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.33it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.40it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.42it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.39it/s]\u001b[A\n",
            "100% 29/29 [00:02<00:00,  9.55it/s]\u001b[A\n",
            "{'eval_loss': 0.7311162948608398, 'eval_accuracy': 0.6533477306365967, 'eval_runtime': 3.0816, 'eval_samples_per_second': 300.496, 'eval_steps_per_second': 9.411, 'epoch': 1.77}\n",
            "\n",
            " 20% 500/2500 [02:51<10:48,  3.08it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 20:55:39,980 >> Saving model checkpoint to out/distilbert/checkpoint-500\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 20:55:39,980 >> Configuration saved in out/distilbert/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 20:55:40,545 >> Model weights saved in out/distilbert/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 20:55:40,546 >> tokenizer config file saved in out/distilbert/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 20:55:40,546 >> Special tokens file saved in out/distilbert/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.6271, 'learning_rate': 1.5200000000000002e-05, 'epoch': 2.13}\n",
            "{'loss': 0.5622, 'learning_rate': 1.4400000000000001e-05, 'epoch': 2.48}\n",
            " 30% 750/2500 [04:15<09:33,  3.05it/s][INFO|trainer.py:710] 2023-02-13 20:57:04,133 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 20:57:04,135 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 20:57:04,135 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 20:57:04,136 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 17.90it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.48it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.27it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02,  9.71it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:02,  9.48it/s]\u001b[A\n",
            " 38% 11/29 [00:01<00:01,  9.38it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.32it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.23it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.20it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.20it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.12it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.12it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.05it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.05it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.00it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.02it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  8.88it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.00it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.08it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.19it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.23it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.21it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.25it/s]\u001b[A\n",
            "100% 29/29 [00:03<00:00,  9.46it/s]\u001b[A\n",
            "{'eval_loss': 0.774253785610199, 'eval_accuracy': 0.6727861762046814, 'eval_runtime': 3.1814, 'eval_samples_per_second': 291.066, 'eval_steps_per_second': 9.115, 'epoch': 2.66}\n",
            "\n",
            " 30% 750/2500 [04:18<09:33,  3.05it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 20:57:07,318 >> Saving model checkpoint to out/distilbert/checkpoint-750\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 20:57:07,318 >> Configuration saved in out/distilbert/checkpoint-750/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 20:57:07,912 >> Model weights saved in out/distilbert/checkpoint-750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 20:57:07,912 >> tokenizer config file saved in out/distilbert/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 20:57:07,912 >> Special tokens file saved in out/distilbert/checkpoint-750/special_tokens_map.json\n",
            "{'loss': 0.5592, 'learning_rate': 1.3600000000000002e-05, 'epoch': 2.84}\n",
            "{'loss': 0.4916, 'learning_rate': 1.2800000000000001e-05, 'epoch': 3.19}\n",
            "{'loss': 0.4539, 'learning_rate': 1.2e-05, 'epoch': 3.55}\n",
            " 40% 1000/2500 [05:42<08:07,  3.07it/s][INFO|trainer.py:710] 2023-02-13 20:58:31,245 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 20:58:31,246 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 20:58:31,247 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 20:58:31,247 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 18.95it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.78it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.59it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02, 10.07it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:01,  9.78it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.58it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.56it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.50it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.46it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.46it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.47it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.41it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.39it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.43it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.41it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.34it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.41it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.33it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.34it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.36it/s]\u001b[A\n",
            "100% 29/29 [00:02<00:00,  9.43it/s]\u001b[A\n",
            "{'eval_loss': 0.8498328328132629, 'eval_accuracy': 0.6565874814987183, 'eval_runtime': 3.0893, 'eval_samples_per_second': 299.749, 'eval_steps_per_second': 9.387, 'epoch': 3.55}\n",
            "\n",
            " 40% 1000/2500 [05:45<08:07,  3.07it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 20:58:34,337 >> Saving model checkpoint to out/distilbert/checkpoint-1000\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 20:58:34,338 >> Configuration saved in out/distilbert/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 20:58:34,910 >> Model weights saved in out/distilbert/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 20:58:34,911 >> tokenizer config file saved in out/distilbert/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 20:58:34,911 >> Special tokens file saved in out/distilbert/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.462, 'learning_rate': 1.1200000000000001e-05, 'epoch': 3.9}\n",
            "{'loss': 0.3941, 'learning_rate': 1.04e-05, 'epoch': 4.26}\n",
            " 50% 1250/2500 [07:09<06:45,  3.08it/s][INFO|trainer.py:710] 2023-02-13 20:59:58,392 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 20:59:58,394 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 20:59:58,394 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 20:59:58,394 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 18.89it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.49it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.21it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02,  9.73it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:02,  9.48it/s]\u001b[A\n",
            " 38% 11/29 [00:01<00:01,  9.44it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.38it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.28it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.26it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.18it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.17it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.14it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.15it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.13it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.09it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.10it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.01it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.07it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.04it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.05it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.03it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.05it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.05it/s]\u001b[A\n",
            "100% 29/29 [00:03<00:00,  9.25it/s]\u001b[A\n",
            "{'eval_loss': 0.8966814279556274, 'eval_accuracy': 0.6576673984527588, 'eval_runtime': 3.1868, 'eval_samples_per_second': 290.576, 'eval_steps_per_second': 9.1, 'epoch': 4.43}\n",
            "\n",
            " 50% 1250/2500 [07:13<06:45,  3.08it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 21:00:01,582 >> Saving model checkpoint to out/distilbert/checkpoint-1250\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 21:00:01,583 >> Configuration saved in out/distilbert/checkpoint-1250/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 21:00:02,375 >> Model weights saved in out/distilbert/checkpoint-1250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 21:00:02,375 >> tokenizer config file saved in out/distilbert/checkpoint-1250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 21:00:02,375 >> Special tokens file saved in out/distilbert/checkpoint-1250/special_tokens_map.json\n",
            "{'loss': 0.3631, 'learning_rate': 9.600000000000001e-06, 'epoch': 4.61}\n",
            "{'loss': 0.3446, 'learning_rate': 8.8e-06, 'epoch': 4.96}\n",
            "{'loss': 0.2826, 'learning_rate': 8.000000000000001e-06, 'epoch': 5.32}\n",
            " 60% 1500/2500 [08:37<05:24,  3.09it/s][INFO|trainer.py:710] 2023-02-13 21:01:25,740 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 21:01:25,741 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 21:01:25,741 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 21:01:25,741 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 18.57it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.89it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.53it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02, 10.08it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:01,  9.77it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.58it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.54it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.55it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.52it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.47it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.43it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.44it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.35it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.33it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.34it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.36it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.28it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.34it/s]\u001b[A\n",
            "{'eval_loss': 0.9745376706123352, 'eval_accuracy': 0.6717062592506409, 'eval_runtime': 3.0886, 'eval_samples_per_second': 299.812, 'eval_steps_per_second': 9.389, 'epoch': 5.32}\n",
            "\n",
            " 60% 1500/2500 [08:40<05:24,  3.09it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 21:01:28,831 >> Saving model checkpoint to out/distilbert/checkpoint-1500\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 21:01:28,832 >> Configuration saved in out/distilbert/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 21:01:29,401 >> Model weights saved in out/distilbert/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 21:01:29,402 >> tokenizer config file saved in out/distilbert/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 21:01:29,402 >> Special tokens file saved in out/distilbert/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 21:01:31,417 >> Deleting older checkpoint [out/distilbert/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 0.2865, 'learning_rate': 7.2000000000000005e-06, 'epoch': 5.67}\n",
            "{'loss': 0.2707, 'learning_rate': 6.4000000000000006e-06, 'epoch': 6.03}\n",
            " 70% 1750/2500 [10:04<04:03,  3.08it/s][INFO|trainer.py:710] 2023-02-13 21:02:52,659 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 21:02:52,661 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 21:02:52,661 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 21:02:52,661 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 18.86it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.92it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.52it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02, 10.06it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:01,  9.81it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.68it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.54it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.52it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.44it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.42it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.42it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.37it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.34it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.24it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.15it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.18it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.14it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.17it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.19it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.14it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.15it/s]\u001b[A\n",
            "100% 29/29 [00:03<00:00,  9.29it/s]\u001b[A\n",
            "{'eval_loss': 1.0345485210418701, 'eval_accuracy': 0.647948145866394, 'eval_runtime': 3.1203, 'eval_samples_per_second': 296.763, 'eval_steps_per_second': 9.294, 'epoch': 6.21}\n",
            "\n",
            " 70% 1750/2500 [10:07<04:03,  3.08it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 21:02:55,783 >> Saving model checkpoint to out/distilbert/checkpoint-1750\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 21:02:55,784 >> Configuration saved in out/distilbert/checkpoint-1750/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 21:02:56,565 >> Model weights saved in out/distilbert/checkpoint-1750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 21:02:56,566 >> tokenizer config file saved in out/distilbert/checkpoint-1750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 21:02:56,566 >> Special tokens file saved in out/distilbert/checkpoint-1750/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 21:02:58,519 >> Deleting older checkpoint [out/distilbert/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.2223, 'learning_rate': 5.600000000000001e-06, 'epoch': 6.38}\n",
            "{'loss': 0.2269, 'learning_rate': 4.800000000000001e-06, 'epoch': 6.74}\n",
            "{'loss': 0.2211, 'learning_rate': 4.000000000000001e-06, 'epoch': 7.09}\n",
            " 80% 2000/2500 [11:31<02:44,  3.04it/s][INFO|trainer.py:710] 2023-02-13 21:04:19,854 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 21:04:19,857 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 21:04:19,857 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 21:04:19,857 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 17.89it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.60it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02,  9.99it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:01,  9.72it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.58it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.57it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.55it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.47it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.45it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.47it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.44it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.38it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.30it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.33it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.40it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.34it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.30it/s]\u001b[A\n",
            "100% 29/29 [00:02<00:00,  9.46it/s]\u001b[A\n",
            "{'eval_loss': 1.1089171171188354, 'eval_accuracy': 0.6511878967285156, 'eval_runtime': 3.1026, 'eval_samples_per_second': 298.464, 'eval_steps_per_second': 9.347, 'epoch': 7.09}\n",
            "\n",
            " 80% 2000/2500 [11:34<02:44,  3.04it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 21:04:22,961 >> Saving model checkpoint to out/distilbert/checkpoint-2000\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 21:04:22,962 >> Configuration saved in out/distilbert/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 21:04:23,541 >> Model weights saved in out/distilbert/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 21:04:23,542 >> tokenizer config file saved in out/distilbert/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 21:04:23,542 >> Special tokens file saved in out/distilbert/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 21:04:25,643 >> Deleting older checkpoint [out/distilbert/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.1869, 'learning_rate': 3.2000000000000003e-06, 'epoch': 7.45}\n",
            "{'loss': 0.1886, 'learning_rate': 2.4000000000000003e-06, 'epoch': 7.8}\n",
            " 90% 2250/2500 [12:58<01:21,  3.07it/s][INFO|trainer.py:710] 2023-02-13 21:05:47,205 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 21:05:47,207 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 21:05:47,207 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 21:05:47,207 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 19.12it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.79it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.57it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02, 10.04it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:01,  9.78it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.59it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.57it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.53it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.48it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.49it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.49it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.46it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.38it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.40it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.40it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.34it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.30it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.29it/s]\u001b[A\n",
            "100% 29/29 [00:02<00:00,  9.46it/s]\u001b[A\n",
            "{'eval_loss': 1.1644123792648315, 'eval_accuracy': 0.6447083950042725, 'eval_runtime': 3.0963, 'eval_samples_per_second': 299.071, 'eval_steps_per_second': 9.366, 'epoch': 7.98}\n",
            "\n",
            " 90% 2250/2500 [13:01<01:21,  3.07it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 21:05:50,305 >> Saving model checkpoint to out/distilbert/checkpoint-2250\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 21:05:50,306 >> Configuration saved in out/distilbert/checkpoint-2250/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 21:05:50,863 >> Model weights saved in out/distilbert/checkpoint-2250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 21:05:50,864 >> tokenizer config file saved in out/distilbert/checkpoint-2250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 21:05:50,864 >> Special tokens file saved in out/distilbert/checkpoint-2250/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 21:05:52,952 >> Deleting older checkpoint [out/distilbert/checkpoint-1250] due to args.save_total_limit\n",
            "{'loss': 0.177, 'learning_rate': 1.6000000000000001e-06, 'epoch': 8.16}\n",
            "{'loss': 0.1634, 'learning_rate': 8.000000000000001e-07, 'epoch': 8.51}\n",
            "{'loss': 0.1648, 'learning_rate': 0.0, 'epoch': 8.87}\n",
            "100% 2500/2500 [14:26<00:00,  3.08it/s][INFO|trainer.py:710] 2023-02-13 21:07:14,480 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 21:07:14,482 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 21:07:14,482 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 21:07:14,482 >>   Batch size = 32\n",
            "\n",
            "  0% 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/29 [00:00<00:01, 18.24it/s]\u001b[A\n",
            " 14% 4/29 [00:00<00:02, 11.45it/s]\u001b[A\n",
            " 21% 6/29 [00:00<00:02, 10.15it/s]\u001b[A\n",
            " 28% 8/29 [00:00<00:02,  9.74it/s]\u001b[A\n",
            " 34% 10/29 [00:00<00:01,  9.51it/s]\u001b[A\n",
            " 38% 11/29 [00:01<00:01,  9.41it/s]\u001b[A\n",
            " 41% 12/29 [00:01<00:01,  9.34it/s]\u001b[A\n",
            " 45% 13/29 [00:01<00:01,  9.25it/s]\u001b[A\n",
            " 48% 14/29 [00:01<00:01,  9.21it/s]\u001b[A\n",
            " 52% 15/29 [00:01<00:01,  9.16it/s]\u001b[A\n",
            " 55% 16/29 [00:01<00:01,  9.19it/s]\u001b[A\n",
            " 59% 17/29 [00:01<00:01,  9.26it/s]\u001b[A\n",
            " 62% 18/29 [00:01<00:01,  9.30it/s]\u001b[A\n",
            " 66% 19/29 [00:01<00:01,  9.29it/s]\u001b[A\n",
            " 69% 20/29 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 72% 21/29 [00:02<00:00,  9.40it/s]\u001b[A\n",
            " 76% 22/29 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 79% 23/29 [00:02<00:00,  9.36it/s]\u001b[A\n",
            " 83% 24/29 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 86% 25/29 [00:02<00:00,  9.32it/s]\u001b[A\n",
            " 90% 26/29 [00:02<00:00,  9.26it/s]\u001b[A\n",
            " 93% 27/29 [00:02<00:00,  9.33it/s]\u001b[A\n",
            " 97% 28/29 [00:02<00:00,  9.38it/s]\u001b[A\n",
            "{'eval_loss': 1.1786128282546997, 'eval_accuracy': 0.6522678136825562, 'eval_runtime': 3.1346, 'eval_samples_per_second': 295.41, 'eval_steps_per_second': 9.252, 'epoch': 8.87}\n",
            "\n",
            "100% 2500/2500 [14:29<00:00,  3.08it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2709] 2023-02-13 21:07:17,618 >> Saving model checkpoint to out/distilbert/checkpoint-2500\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 21:07:17,619 >> Configuration saved in out/distilbert/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 21:07:18,159 >> Model weights saved in out/distilbert/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 21:07:18,160 >> tokenizer config file saved in out/distilbert/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 21:07:18,160 >> Special tokens file saved in out/distilbert/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|trainer.py:2787] 2023-02-13 21:07:20,279 >> Deleting older checkpoint [out/distilbert/checkpoint-1500] due to args.save_total_limit\n",
            "[INFO|trainer.py:1901] 2023-02-13 21:07:20,345 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2025] 2023-02-13 21:07:20,345 >> Loading best model from out/distilbert/checkpoint-750 (score: 0.6727861762046814).\n",
            "{'train_runtime': 873.3396, 'train_samples_per_second': 91.602, 'train_steps_per_second': 2.863, 'train_loss': 0.4143409591674805, 'epoch': 8.87}\n",
            "100% 2500/2500 [14:33<00:00,  2.86it/s]\n",
            "[INFO|trainer.py:2709] 2023-02-13 21:07:21,727 >> Saving model checkpoint to out/distilbert\n",
            "[INFO|configuration_utils.py:453] 2023-02-13 21:07:21,728 >> Configuration saved in out/distilbert/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-13 21:07:22,272 >> Model weights saved in out/distilbert/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-13 21:07:22,273 >> tokenizer config file saved in out/distilbert/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-13 21:07:22,273 >> Special tokens file saved in out/distilbert/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       8.87\n",
            "  train_loss               =     0.4143\n",
            "  train_runtime            = 0:14:33.33\n",
            "  train_samples            =       9000\n",
            "  train_samples_per_second =     91.602\n",
            "  train_steps_per_second   =      2.863\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:710] 2023-02-13 21:07:22,311 >> The following columns in the evaluation set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 21:07:22,313 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 21:07:22,313 >>   Num examples = 926\n",
            "[INFO|trainer.py:2969] 2023-02-13 21:07:22,313 >>   Batch size = 32\n",
            "100% 29/29 [00:02<00:00,  9.71it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       8.87\n",
            "  eval_accuracy           =     0.6728\n",
            "  eval_loss               =     0.7743\n",
            "  eval_runtime            = 0:00:03.13\n",
            "  eval_samples            =        926\n",
            "  eval_samples_per_second =    295.107\n",
            "  eval_steps_per_second   =      9.242\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:710] 2023-02-13 21:07:25,455 >> The following columns in the test set don't have a corresponding argument in `DistilbertCustomClassifier.forward` and have been ignored: text. If text are not expected by `DistilbertCustomClassifier.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-13 21:07:25,456 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2966] 2023-02-13 21:07:25,456 >>   Num examples = 27742\n",
            "[INFO|trainer.py:2969] 2023-02-13 21:07:25,457 >>   Batch size = 32\n",
            "100% 867/867 [01:33<00:00,  9.30it/s]\n",
            "INFO:__main__:***** Predict results None *****\n",
            "[INFO|modelcard.py:449] 2023-02-13 21:08:59,670 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6727861762046814}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1kmt52uekov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}